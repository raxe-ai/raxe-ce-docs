---
title: "LangChain Integration"
description: "Think-time security for LangChain agents, chains, and tools"
---

## Overview

RAXE provides **think-time security** for LangChain agents â€” real-time threat detection during inference, before action execution. Protect chains, ReAct agents, tools, memory, and RAG pipelines.

**What RAXE scans:**
- Agent prompts and reasoning
- Tool call requests and results
- Memory content retrieval
- RAG context injection

## Installation

```bash
pip install raxe[langchain]
```

## Callback Handler

Use the RAXE callback handler to scan prompts and responses:

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from raxe.sdk.integrations.langchain import RaxeCallbackHandler

# Create callback handler (default: log-only mode)
handler = RaxeCallbackHandler()

# Use with LangChain
llm = ChatOpenAI(
    model="gpt-4",
    callbacks=[handler]
)

# Scans are automatic
response = llm.invoke([HumanMessage(content="Hello, how are you?")])
```

## Configuration Options

```python
from raxe.sdk.agent_scanner import ToolPolicy

handler = RaxeCallbackHandler(
    # Blocking behavior (default: log-only, no blocking)
    block_on_prompt_threats=True,    # Block if prompt threat detected
    block_on_response_threats=True,  # Block if response threat detected

    # What to scan
    scan_tools=True,           # Scan tool inputs/outputs (default: True)
    scan_agent_actions=True,   # Scan agent actions (default: True)

    # Tool restrictions
    tool_policy=ToolPolicy.block_tools("shell", "file_write"),

    # Optional threat callback
    on_threat=lambda result: print(f"Threat: {result.severity}"),
)
```

## Chain Integration

```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from raxe.sdk.integrations.langchain import RaxeCallbackHandler

# Setup - blocking mode for prompts
handler = RaxeCallbackHandler(block_on_prompt_threats=True)
llm = ChatOpenAI(model="gpt-4")

prompt = PromptTemplate(
    input_variables=["question"],
    template="Answer this question: {question}"
)

chain = LLMChain(
    llm=llm,
    prompt=prompt,
    callbacks=[handler]
)

# All inputs are scanned
result = chain.run(question="What is machine learning?")
```

## Agent Integration

```python
from langchain.agents import create_react_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain import hub
from raxe.sdk.integrations.langchain import RaxeCallbackHandler
from raxe.sdk.agent_scanner import ToolPolicy

# Create handler with tool restrictions
handler = RaxeCallbackHandler(
    block_on_prompt_threats=True,
    tool_policy=ToolPolicy.block_tools("shell", "execute_command"),
)

# Setup agent
llm = ChatOpenAI(model="gpt-4")
prompt = hub.pull("hwchase17/react")
tools = []  # Your tools here

agent = create_react_agent(llm, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    callbacks=[handler]
)

# Execute with protection
result = agent_executor.invoke({"input": "Hello"})
```

## RAG Protection

Protect RAG pipelines from data exfiltration:

```python
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from raxe.sdk.integrations.langchain import RaxeCallbackHandler

# Create handler with RAG focus
handler = RaxeCallbackHandler(
    block_on_prompt_threats=True,
    block_on_response_threats=True,
)

# Setup RAG chain
llm = ChatOpenAI(model="gpt-4")
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(embedding_function=embeddings)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    callbacks=[handler]
)

# Protected queries
result = qa_chain.invoke({"query": "What are our policies?"})
```

## Error Handling

```python
from raxe.sdk.exceptions import SecurityException
from raxe.sdk.integrations.langchain import RaxeCallbackHandler

handler = RaxeCallbackHandler(block_on_prompt_threats=True)

try:
    result = chain.run(question=user_input)
except SecurityException as e:
    print(f"Blocked due to security threat")
    # Handle blocked request appropriately
```

## Tool Policy

Restrict which tools agents can use:

```python
from raxe.sdk.agent_scanner import ToolPolicy
from raxe.sdk.integrations.langchain import RaxeCallbackHandler

# Block specific dangerous tools
handler = RaxeCallbackHandler(
    tool_policy=ToolPolicy.block_tools("shell", "file_write", "execute_code")
)

# Or only allow specific tools
handler = RaxeCallbackHandler(
    tool_policy=ToolPolicy.allow_tools("search", "calculator", "weather")
)
```

## Best Practices

<AccordionGroup>
  <Accordion title="Start with log-only mode">
    Begin with monitoring before enabling blocking:
    ```python
    # Default: log-only (no blocking)
    handler = RaxeCallbackHandler()

    # Later, enable blocking after tuning
    handler = RaxeCallbackHandler(
        block_on_prompt_threats=True,
        block_on_response_threats=True,
    )
    ```
  </Accordion>
  <Accordion title="Use tool policies for agents">
    Restrict dangerous tools to prevent security issues:
    ```python
    handler = RaxeCallbackHandler(
        tool_policy=ToolPolicy.block_tools("shell", "file_write")
    )
    ```
  </Accordion>
  <Accordion title="Handle blocked requests gracefully">
    Always catch `SecurityException` for user-friendly responses:
    ```python
    from raxe.sdk.exceptions import SecurityException

    try:
        result = chain.run(user_input)
    except SecurityException:
        return "I can't process that request."
    ```
  </Accordion>
</AccordionGroup>

## Supported LangChain Versions

| LangChain Version | Status |
|-------------------|--------|
| 0.1.x | Supported |
| 0.2.x | Supported |
| 0.3.x | Supported |
